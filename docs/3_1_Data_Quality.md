# Step 1: Filtering data without Augmentation

### GHCN-Daily Data Processing

This project processes GHCN-Daily weather data using Hadoop MapReduce, focusing on filtering records from 1974 to 2024 for specific elements (TMAX, TMIN, PRCP) and outputting them without augmentation.

### Components

- **Mapper (`GHCNMapper.java`)**: Filters records by year and element.
- **Reducer (`GHCNReducer.java`)**: Passes filtered records directly to output.
- **Driver (`GHCNDataProcessor.java`)**: Configures and runs the MapReduce job.

### Running the Job

Compile the Java classes into a JAR and run the MapReduce job with:

```sh
hadoop jar /opt/hadoop-3.2.1/share/hadoop/mapreduce/DataFiltering-0.0.1-SNAPSHOT.jar com.example.GHCNDataProcessor user/root/input/ user/root/output1/
```

- `<input_path>`: HDFS path to GHCN-Daily data.
- `<output_path>`: HDFS path for job output (must not exist prior).

### output is stored

`sh hadoop fs -cat output1/*`

### Output Format

The output is a CSV with columns: StationID, Date, ElementType, Value.
![image](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/3349c2e1-001c-4665-8a9d-08fa97b6268a)

### Step 2: Preprocessing GHCN Stations Data

We need state name, location for each of previous station id's , that we can able to grab only from ghcnd_stations.txt and ghcnd-states.txt so this helper job removes unwanted records from ghncd-stations.txt. This is helpful for future augmentation

### Components

- **Mapper (`GhcndStationMapper.java`)**: Processes station data to replace state codes with state names for USA and Canada, and formats records. Parse logic is different according to country because for mexico we dont have statecode
- **Reducer (`GhcndStationReducer.java`)**: Aggregates processed data.
- **Driver (`GhcndStationDriver.java`)**: Manages job execution.

### Execution

To execute, compile into a JAR and run with:

```sh
hadoop jar GhcndStationsFilter-0.0.1-SNAPSHOT.jar com.example.GhcndStationDriver metadatafiles/ghcnd-stations.txt meta metadatafiles/ghcnd-states.txt
```

- `metadata/ghcnd-stations.txt`: Is the file that we have to parse.
- `meta`: Destination for preprocessed data.
- `ghcnd-states.txt` is used for converting state code to actual state name

We will rename the file generated by this map reduce job

```sh
 hadoop fs -mv meta/part-r-00000 metadatafiles/ghcnd_stations_updated.csv
```

### Output

The output is formatted as CSV with columns: StationID, StateName, LocationName, Country, ensuring data is ready for detailed analysis with enhanced geographic context.
![meta_data](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/9fc1b210-21f2-4206-8948-dab9d5d04a79)


Step 3 : Augmentation of data
```sh
hadoop jar DataFilteringAndAugmentation-0.0.1-SNAPSHOT.jar com.example.AugmentJobDriver user/root/output1/part-r-00000 user/root/output2 metadatafiles/ghcnd_stations_updated.csv
```
![out2](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/050dfe06-d3f2-4c78-ba1e-59e0c42dd521)


Step 4: Sorting Data
```sh
hadoop jar SortingData-0.0.1-SNAPSHOT.jar com.example.SortingJobDriver user/root/output2/part-r-00000 user/root/output3
```
![image](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/81bcdeba-915f-483e-8537-194ba59be169)



Step 5: Discarding Data which are older than 10 days
  Before this old delete as because its going to increase space 
  Trying to delete older outputs just to save space
  ```sh
  hadoop fs -rm -R user/root/output1
  ```
  ```sh
  hadoop fs -rm -R user/root/output2
  ```
```sh
hadoop jar DiscardMissingData-0.0.1-SNAPSHOT.jar com.example.ElementGapDriver user/root/output3/part-r-00000 user/root/output4
```
![image](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/e028aab8-9faa-4084-adfe-1d0e92d0a4fd)



Step 6: Imputation of Temperature

```sh 
 hadoop jar DataImputationAndAggregation-0.0.1-SNAPSHOT.jar com.example.TemperatureDataProcessingDriver user/root/output4/part-r-00000 user/root/output5
```
Also the result is now changed output will look like : station id, date, tmin value,tmax value,tavg value,prcp value,state,location,country

![image](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/43f7fa34-bbeb-4772-9a9b-4e1fab800830)



Step 7 : Imputation of Precipitation data
```sh 
   hadoop jar ImputationPrecipitation-0.0.1-SNAPSHOT.jar com.example.PrecipitationDriver user/root/output5/part-r-00000 user/root/output6
```
![image](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/e100e802-105d-4bc3-bb3e-5de913e9f31f)


Step 8: Temperature Outliers
```sh
   hadoop jar TemperatureOutliers-0.0.1-SNAPSHOT.jar com.example.TemperatureOutlierDriver user/root/output6/part-r-00000 user/root/output7
```
![image](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/9df8bf74-e78c-40ff-84d6-10ec06a297c7)


Step 9: Percipitation Outliers
``` sh
  hadoop jar PercipitationOutliers-0.0.1-SNAPSHOT.jar com.example.PercipitationOutlierDriver user/root/output7/part-r-00000 user/root/output8
```
![image](https://github.com/aravind2060/HadoopAndHiveforLargeScaleDataAnalysis/assets/38257404/beb3eeb6-88c2-4e69-9d31-5d7b05b57281)

Now we have final data which is free from outliers, now for further analysis we have to use this data 
```sh
hadoop fs -cat user/root/output8/*
```

